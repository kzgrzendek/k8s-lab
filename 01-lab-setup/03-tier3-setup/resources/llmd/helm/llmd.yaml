multinode: false

modelArtifacts:
  name: "Qwen/Qwen3-0.6B"
  uri: "hf://Qwen/Qwen3-0.6B"
  size: "8Gi"

  labels:
    llm-d.ai/inferenceServing: "true"
    llm-d.ai/model: "qwen3-0-6b"

routing:
  servicePort: 8000

  proxy:
    image: ghcr.io/llm-d/llm-d-routing-sidecar:v0.4.0-rc.1

    targetPort: 8200
    secure: false
    debugLevel: 1

decode:
  create: true
  replicas: 1

  containers:
    - name: vllm
      image: ghcr.io/llm-d/llm-d-cuda:v0.4.0
      modelCommand: vllmServe
      args:
        - Qwen/Qwen3-0.6B
        - --port
        - "8200"
        - --served-model-name
        - Qwen/Qwen3-0.6B
        - --max-model-len
        - "32768"

      ports:
        - containerPort: 8200
          protocol: TCP

      mountModelVolume: true

      resources:
        limits:
          nvidia.com/gpu: "1"
        requests:
          nvidia.com/gpu: "1"

      startupProbe:
        httpGet:
          path: /v1/models
          port: 8200
        initialDelaySeconds: 15
        periodSeconds: 30
        timeoutSeconds: 5
        failureThreshold: 60
      
      livenessProbe:
        httpGet:
          path: /health
          port: 8200
        periodSeconds: 10
        timeoutSeconds: 5
        failureThreshold: 3
      
      readinessProbe:
        httpGet:
          path: /v1/models
          port: 8200
        periodSeconds: 5
        timeoutSeconds: 2
        failureThreshold: 3

prefill:
  create: false
