inferencePool:
  name: llmd-qwen3-pool
  namespace: llmd

  apiVersion: inference.networking.k8s.io/v1

  modelServerType: vllm

  modelName: Qwen/Qwen3-0.6B

  targetPorts:
    - number: 8200

  modelServers:
    matchLabels:
      "llm-d.ai/inferenceServing": "true"
      "llm-d.ai/model": qwen3-0-6b
      "llm-d.ai/role": decode

endpointPicker:
  args:
    - --secure-serving=false

provider:
  name: none
