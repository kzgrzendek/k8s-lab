multinode: false

modelArtifacts:
  name: "{{.ModelName}}"
  uri: "hf://{{.ModelName}}"
  size: "8Gi"
  # Use pre-warmed model from PVC if available, otherwise llmd downloads on startup
  existingClaim: "{{.ModelPVCName}}"

  labels:
    llm-d.ai/inferenceServing: "true"
    llm-d.ai/model: "{{.ModelSlug}}"

routing:
  servicePort: 8000

  proxy:
    image: ghcr.io/llm-d/llm-d-routing-sidecar:v0.4.0-rc.1

    targetPort: 8200
    secure: false
    debugLevel: 1

decode:
  create: true
  replicas: 1

  nodeSelector:
    nova.local/llmd-node: "true"

  containers:
    - name: vllm
      image: ghcr.io/llm-d/llm-d-cuda:v0.4.0
      modelCommand: vllmServe
      args:
        - {{.ModelName}}
        - --port
        - "8200"
        - --served-model-name
        - {{.ModelName}}
        - --max-model-len
        - "32768"

      env:
        - name: HF_TOKEN
          valueFrom:
            secretKeyRef:
              name: huggingface-token
              key: token
              optional: true

      ports:
        - containerPort: 8200
          protocol: TCP

      mountModelVolume: true

      resources:
        limits:
          nvidia.com/gpu: "1"
        requests:
          nvidia.com/gpu: "1"

      startupProbe:
        httpGet:
          path: /v1/models
          port: 8200
        initialDelaySeconds: 15
        periodSeconds: 30
        timeoutSeconds: 5
        failureThreshold: 60
      
      livenessProbe:
        httpGet:
          path: /health
          port: 8200
        periodSeconds: 10
        timeoutSeconds: 5
        failureThreshold: 3
      
      readinessProbe:
        httpGet:
          path: /v1/models
          port: 8200
        periodSeconds: 5
        timeoutSeconds: 2
        failureThreshold: 3

prefill:
  create: false
