multinode: false

# Disable GPU accelerator for CPU mode
accelerator:
  type: ""

modelArtifacts:
  name: "{{.ModelName}}"
  # Use pre-warmed model from NFS-backed PVC if available
  # Format: pvc://pvc_name/path/to/model
  uri: "pvc://{{.ModelPVCName}}/"
  size: "8Gi"

  labels:
    llm-d.ai/inferenceServing: "true"
    llm-d.ai/model: "{{.ModelSlug}}"

routing:
  servicePort: 8000

  proxy:
    image: ghcr.io/llm-d/llm-d-routing-sidecar:v0.4.0-rc.1

    targetPort: 8200
    secure: false
    debugLevel: 1

decode:
  create: true
  replicas: 1

  nodeSelector:
    nova.local/llmd-node: "true"

  containers:
    - name: vllm
      image: "{{.LLMDCpuImage}}"
      modelCommand: vllmServe
      args:
        - --max-model-len
        - "32768"

      env:
        - name: HF_TOKEN
          valueFrom:
            secretKeyRef:
              name: huggingface-token
              key: token
              optional: true

      ports:
        - containerPort: 8200
          protocol: TCP

      mountModelVolume: true

      resources:
        limits:
          cpu: "8"
          memory: "8Gi"
        requests:
          cpu: "4"
          memory: "4Gi"

      startupProbe:
        httpGet:
          path: /v1/models
          port: 8200
        initialDelaySeconds: 15
        periodSeconds: 30
        timeoutSeconds: 5
        failureThreshold: 60

      livenessProbe:
        httpGet:
          path: /health
          port: 8200
        periodSeconds: 10
        timeoutSeconds: 5
        failureThreshold: 3

      readinessProbe:
        httpGet:
          path: /v1/models
          port: 8200
        periodSeconds: 5
        timeoutSeconds: 2
        failureThreshold: 3

prefill:
  create: false
